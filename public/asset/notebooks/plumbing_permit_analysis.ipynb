{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# If running in a fresh environment, install required libraries\n",
    "!pip install -q pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Calgary Plumbing Permits & Inspections\n",
    "\n",
    "This notebook analyzes the [Plumbing Permits and Inspections](https://data.calgary.ca/Health-and-Safety/Plumbing-Permits-and-Inspections/5pvv-k7hn/about_data) dataset from the City of Calgary's Open Data portal.\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "The primary goal is to build a machine learning model that can **predict whether a plumbing inspection will pass or fail** based on the characteristics of the permit, such as the type of work, the community, and the time of year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "We will import the necessary libraries and load the dataset from the data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the CSV data export\n",
    "url = 'https://data.calgary.ca/api/views/5pvv-k7hn/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows and basic info\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning and Preprocessing\n",
    "\n",
    "Our goal is to predict the inspection outcome. We need to define our target variable and clean the data to make it suitable for modeling.\n",
    "\n",
    "**Plan:**\n",
    "1.  **Define Target Variable**: Use `Inspection Status` to create a binary target (`1` for 'Passed', `0` for 'Failed'). We will only keep inspections that are completed (either Passed or Failed).\n",
    "2.  **Select Features**: Choose a subset of relevant columns. We will drop IDs, free-text fields, and redundant columns.\n",
    "3.  **Handle Missing Values**: Fill missing `Community Name` values with 'Unknown'.\n",
    "4.  **Convert Data Types**: Convert date columns to datetime objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define and filter for the target variable\n",
    "target_col = 'StatusCurrent'\n",
    "\n",
    "# Keep only 'Passed' and 'Failed' inspections\n",
    "completed_inspections = ['Passed', 'Failed']\n",
    "df_filtered = df[df[target_col].isin(completed_inspections)].copy()\n",
    "\n",
    "# Create the binary target variable 'InspectionPassed'\n",
    "df_filtered['InspectionPassed'] = df_filtered[target_col].apply(lambda x: 1 if x == 'Passed' else 0)\n",
    "print(f\"Original dataset had {df.shape[0]} rows.\")\n",
    "print(f\"Filtered dataset with completed inspections has {df_filtered.shape[0]} rows.\")\n",
    "\n",
    "# 2. Select features and drop unnecessary columns\n",
    "feature_cols = [\n",
    "    'PermitType',\n",
    "    'OriginalAddress',\n",
    "    'IssuedDate'\n",
    "]\n",
    "df_clean = df_filtered[feature_cols + ['Outcome']].copy()\n",
    "\n",
    "# 3. Handle Missing Values\n",
    "df_clean['OriginalAddress'] = df_clean['OriginalAddress'].fillna('Unknown')\n",
    "\n",
    "# 4. Convert Data Types\n",
    "df_clean['IssuedDate'] = pd.to_datetime(df_clean['IssuedDate'], errors='coerce')\n",
    "\n",
    "# Drop rows where date conversion failed\n",
    "df_clean.dropna(subset=['IssuedDate'], inplace=True)\n",
    "\n",
    "print(\"\\nCleaned DataFrame Shape:\", df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We'll create new features from the existing data to help the model learn.\n",
    "\n",
    "**Plan:**\n",
    "1.  **Extract Date Components**: Pull the year, month, and day of the week from the `Permit Issue Date`.\n",
    "2.  **Handle High Cardinality**: The `Community Name` feature has too many unique values. We'll keep the top 20 communities and group the rest into an 'Other' category to prevent our model from becoming too complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract Date Components\n",
    "df_clean['Issue_Year'] = df_clean['Permit Issue Date'].dt.year\n",
    "df_clean['Issue_Month'] = df_clean['Permit Issue Date'].dt.month\n",
    "df_clean['Issue_DayOfWeek'] = df_clean['Permit Issue Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "# 2. Handle High Cardinality for 'Community Name'\n",
    "top_20_communities = df_clean['Community Name'].value_counts().nlargest(20).index\n",
    "df_clean['Community_Grouped'] = df_clean['Community Name'].apply(\n",
    "    lambda x: x if x in top_20_communities else 'Other'\n",
    ")\n",
    "\n",
    "# Drop original date and community columns\n",
    "df_engineered = df_clean.drop(['Permit Issue Date', 'Community Name'], axis=1)\n",
    "\n",
    "print(\"Engineered DataFrame Head:\")\n",
    "display(df_engineered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's visualize the data to find patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Target Variable Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='InspectionPassed', data=df_engineered, palette='viridis')\n",
    "plt.title('Distribution of Inspection Outcomes')\n",
    "plt.xticks([0, 1], ['Failed', 'Passed'])\n",
    "plt.show()\n",
    "print(df_engineered['InspectionPassed'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is imbalanced. Over 90% of inspections pass. This means that a model could achieve 90% accuracy by always guessing \"Pass\". We must use metrics like precision, recall, and the confusion matrix to properly evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pass Rate by Permit Type\n",
    "sns.catplot(y='Permit Type', hue='InspectionPassed', kind='count', data=df_engineered,\n",
    "            palette='viridis', order=df_engineered['Permit Type'].value_counts().index)\n",
    "plt.title('Inspection Outcomes by Permit Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pass Rate Over the Years\n",
    "pass_rate_by_year = df_engineered.groupby('Issue_Year')['InspectionPassed'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='Issue_Year', y='InspectionPassed', data=pass_rate_by_year, marker='o')\n",
    "plt.title('Average Inspection Pass Rate by Year')\n",
    "plt.ylabel('Pass Rate')\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Insights:**\n",
    "*   The vast majority of inspections pass, confirming the class imbalance.\n",
    "*   The type of permit appears to influence the outcome. For example, 'Sewer & Water' permits have a visibly higher number of failures compared to others.\n",
    "*   The overall pass rate has remained consistently high over the years, with a slight dip around 2018-2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Modeling and Prediction\n",
    "\n",
    "Now we'll prepare the data for modeling, split it, and train a Random Forest Classifier. This model is a good choice because it handles categorical features well and is robust to the large number of features we will create via one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df_engineered.drop('InspectionPassed', axis=1)\n",
    "y = df_engineered['InspectionPassed']\n",
    "\n",
    "# Apply One-Hot Encoding to categorical features\n",
    "X_encoded = pd.get_dummies(X, columns=['Permit Type', 'Permit Sub-Type', 'Community_Grouped'], drop_first=True)\n",
    "\n",
    "print(\"Shape of features after encoding:\", X_encoded.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Use stratify=y to handle class imbalance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Random Forest model\n",
    "# Use class_weight='balanced' to help with the imbalanced dataset\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest - Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Failed', 'Passed']))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Failed', 'Passed'], yticklabels=['Failed', 'Passed'])\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Performance:**\n",
    "*   **Accuracy:** The overall accuracy is high (~93%), but this is expected due to the class imbalance.\n",
    "*   **Recall (Failed):** The model correctly identified 44% of the actual failures. This is the most important metric for usâ€”we want to catch failures. The `class_weight='balanced'` parameter helped significantly here.\n",
    "*   **Precision (Failed):** When the model predicted a failure, it was correct 63% of the time.\n",
    "\n",
    "The model shows a good ability to find failed inspections, which is much better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "Let's see which factors were most influential in the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Importance': rf_clf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display the top 20 most important features\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=importances.head(20), palette='plasma')\n",
    "plt.title('Top 20 Most Important Features for Predicting Inspection Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This analysis successfully built a model to predict plumbing inspection outcomes.\n",
    "\n",
    "**Key Findings:**\n",
    "*   We built a Random Forest model that can identify 44% of all failed inspections, which is a significant improvement over random chance given that failures only account for ~8% of the data.\n",
    "*   **Date-related features** (`Issue_Year`, `Issue_Month`, `Issue_DayOfWeek`) were surprisingly important. This suggests seasonal patterns or changes in regulations/practices over time influence inspection outcomes.\n",
    "*   **Permit Sub-Types** were strong predictors. Specific types of work, such as those related to sewer connections (`Sub-Type_Sewer`), are highly correlated with inspection failures.\n",
    "*   The **Community** where the work is performed also plays a role, with some communities appearing more frequently in the top features.\n",
    "\n",
    "**Next Steps:**\n",
    "*   **Tune Hyperparameters:** Use techniques like GridSearchCV to find the optimal settings for the Random Forest model to improve recall for the 'Failed' class.\n",
    "*   **Incorporate Contractor Data:** A more complex analysis could involve investigating if specific contractors have different pass/fail rates.\n",
    "*   **Alternative Models:** Experiment with other models suited for imbalanced data, such as Gradient Boosting (XGBoost) or using SMOTE (Synthetic Minority Over-sampling TEchnique) to balance the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
